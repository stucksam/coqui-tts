#!/bin/bash
#SBATCH --time=1440
#SBATCH --job-name=run_tts_coqui_stucksam
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --gres=gpu:2
#SBATCH --cpus-per-task=8
#SBATCH --nodelist=sacramento
#SBATCH --mem=64G
#SBATCH --account=cai_nlp
#SBATCH --partition=p_gpu_all
#SBATCH --output=/cluster/home/stucksam/log/%j_%N__run_tts_coqui_stucksam.out
#SBATCH --error=/cluster/home/stucksam/log/%j_%N__run_tts_coqui_stucksam.err

# %j: parses the slurm jobID into the file-name
# %N: parses the Node name into the file-name

env_name="coqui-tts"
venv_base_dir="/raid/persistent_scratch/stucksam/venvs"
venv_path="$venv_base_dir/$env_name"

# create venv base dir if it does not exist
mkdir -p /raid/persistent_scratch/stucksam/venvs/
#rm -r $venv_path

# Explicitly load cuda 12.2.2
module load cuda/12.2.2

# Check if the virtual environment exists
echo "Searching ($venv_path)..."
if [ -d "$venv_path" ]; then
    echo "Virtual environment ($env_name) found. Activating..."
    source "$venv_path/bin/activate"
else
    echo "Virtual environment ($env_name) not found. Creating..."
    module load python/3.11.9
    virtualenv $venv_path
    unset PIP_TARGET
    unset PYTHONPATH
    source "$venv_path/bin/activate"

    pip3 install -e .[all,dev,notebooks]
    pip3 install wandb h5py
fi

# see https://github.com/pytorch/pytorch/issues/111469
export LD_LIBRARY_PATH=$venv_path/lib64/python3.11/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH

# Source and destination directories
SOURCE_DIR="/cluster/home/stucksam/datasets/dialects"
DEST_DIR="/scratch/dialects"

python3 TTS/TTS_CH/copy_h5_concurrently.py

# Optional: Print a message to indicate completion
echo "Folder copied from $SOURCE_DIR to $DEST_DIR"

# python3 recipes/ljspeech/xtts_v2/train_gpt_xtts_snf.py

python3 -m trainer.distribute --script recipes/ljspeech/xtts_v2/train_gpt_xtts_snf.py  --gpus 0,1

# submit the job:
# sbatch <path-to-submit-file>/main.submit

